{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200b2925",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ Importaciones y configuraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\", \"..\"))\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "\n",
    "TRAIN_CSV = os.path.join(OUTPUT_DIR, \"train_processed.csv\")\n",
    "VAL_CSV   = os.path.join(OUTPUT_DIR, \"val_processed.csv\")\n",
    "TEST_CSV  = os.path.join(OUTPUT_DIR, \"test_processed.csv\")\n",
    "\n",
    "sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from src.part_2.main import run_training\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, name=\"Model\"):\n",
    "    \"\"\"Calculate and print comprehensive metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def study_l2_regularization(TRAIN_CSV, CSV_VAL, CSV_TEST, activation=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Study 1: L2 Regularization\n",
    "    Test different L2 lambda values to find optimal regularization strength\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STUDY 1: L2 REGULARIZATION (WEIGHT DECAY)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nL2 regularization adds a penalty term: Loss = MSE + (Î»/2) Ã— Î£(wÂ²)\")\n",
    "    print(\"This prevents overfitting by penalizing large weights.\\n\")\n",
    "    \n",
    "    # Different L2 lambda values to test\n",
    "    l2_values = [0.0, 0.0001, 0.001, 0.01, 0.1]\n",
    "    results = []\n",
    "    \n",
    "    for l2_lambda in l2_values:\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Testing L2_lambda = {l2_lambda}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        # Run training with specific L2 value\n",
    "        nn, pid_test, y_test, preds_test, trainer = run_training(\n",
    "            activation=activation,\n",
    "            verbose=False,\n",
    "            epochs=200,\n",
    "            CSV_TRAIN=TRAIN_CSV,\n",
    "            CSV_VAL=CSV_VAL,\n",
    "            CSV_TEST=CSV_TEST,\n",
    "            l2_lambda=l2_lambda,\n",
    "            dropout_rate=0.0,  # No dropout for this study\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test, preds_test, f\"L2={l2_lambda}\")\n",
    "        \n",
    "        # Get training history\n",
    "        loss_df = trainer.loss_epochs()\n",
    "        final_train_error = loss_df['Train Error'].iloc[-1]\n",
    "        final_val_error = loss_df['Val Error'].iloc[-1]\n",
    "        train_val_gap = abs(final_val_error - final_train_error)\n",
    "        \n",
    "        metrics['Final_Train_Error'] = final_train_error\n",
    "        metrics['Final_Val_Error'] = final_val_error\n",
    "        metrics['Train_Val_Gap'] = train_val_gap\n",
    "        metrics['L2_Lambda'] = l2_lambda\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  RMSE:           ${metrics['RMSE']:,.2f}\")\n",
    "        print(f\"  MAE:            ${metrics['MAE']:,.2f}\")\n",
    "        print(f\"  RÂ²:             {metrics['R2']:.4f}\")\n",
    "        print(f\"  MAPE:           {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"  Train Error:    {final_train_error:.6f}\")\n",
    "        print(f\"  Val Error:      {final_val_error:.6f}\")\n",
    "        print(f\"  Train-Val Gap:  {train_val_gap:.6f}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"L2 REGULARIZATION - SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df[['L2_Lambda', 'RMSE', 'MAE', 'R2', 'MAPE', 'Train_Val_Gap']].to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = results_df['RMSE'].idxmin()\n",
    "    best_lambda = results_df.loc[best_idx, 'L2_Lambda']\n",
    "    \n",
    "    print(f\"\\nğŸ† BEST L2 LAMBDA: {best_lambda}\")\n",
    "    print(f\"   RMSE: ${results_df.loc[best_idx, 'RMSE']:,.2f}\")\n",
    "    print(f\"   RÂ²:   {results_df.loc[best_idx, 'R2']:.4f}\")\n",
    "    print(f\"   Train-Val Gap: {results_df.loc[best_idx, 'Train_Val_Gap']:.6f}\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"â”€\"*80)\n",
    "    print(\"ANALYSIS:\")\n",
    "    baseline_rmse = results_df[results_df['L2_Lambda'] == 0.0]['RMSE'].values[0]\n",
    "    best_rmse = results_df.loc[best_idx, 'RMSE']\n",
    "    improvement = ((baseline_rmse - best_rmse) / baseline_rmse) * 100\n",
    "    \n",
    "    print(f\"  â€¢ Baseline (no L2) RMSE: ${baseline_rmse:,.2f}\")\n",
    "    print(f\"  â€¢ Best L2 RMSE:          ${best_rmse:,.2f}\")\n",
    "    print(f\"  â€¢ Improvement:           {improvement:.2f}%\")\n",
    "    \n",
    "    baseline_gap = results_df[results_df['L2_Lambda'] == 0.0]['Train_Val_Gap'].values[0]\n",
    "    best_gap = results_df.loc[best_idx, 'Train_Val_Gap']\n",
    "    gap_reduction = ((baseline_gap - best_gap) / baseline_gap) * 100\n",
    "    print(f\"  â€¢ Train-Val Gap reduction: {gap_reduction:.2f}%\")\n",
    "    print(\"â”€\"*80)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(\"l2_regularization_results.csv\", index=False)\n",
    "    print(\"\\nâœ“ Results saved to: l2_regularization_results.csv\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def study_dropout(TRAIN_CSV, CSV_VAL, CSV_TEST, activation=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Study 2: Dropout Regularization\n",
    "    Test different dropout rates to find optimal value\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"STUDY 2: DROPOUT REGULARIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nDropout randomly deactivates neurons during training.\")\n",
    "    print(\"This prevents co-adaptation and acts as an ensemble method.\\n\")\n",
    "    \n",
    "    # Different dropout rates to test\n",
    "    dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "    results = []\n",
    "    \n",
    "    # Use a deeper architecture for dropout (works better with deeper networks)\n",
    "    deeper_layers = [277, 64, 32, 16, 1]\n",
    "    \n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Testing Dropout Rate = {dropout_rate}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        # Run training with specific dropout rate\n",
    "        nn, pid_test, y_test, preds_test, trainer = run_training(\n",
    "            activation=activation,\n",
    "            verbose=False,\n",
    "            epochs=200,\n",
    "            CSV_TRAIN=TRAIN_CSV,\n",
    "            CSV_VAL=CSV_VAL,\n",
    "            CSV_TEST=CSV_TEST,\n",
    "            layers=deeper_layers,  # Use deeper network\n",
    "            lr=0.01,  # Slightly lower LR for dropout\n",
    "            l2_lambda=0.0,  # No L2 for this study\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test, preds_test, f\"Dropout={dropout_rate}\")\n",
    "        \n",
    "        # Get training history\n",
    "        loss_df = trainer.loss_epochs()\n",
    "        final_train_error = loss_df['Train Error'].iloc[-1]\n",
    "        final_val_error = loss_df['Val Error'].iloc[-1]\n",
    "        train_val_gap = abs(final_val_error - final_train_error)\n",
    "        \n",
    "        \n",
    "        metrics['Final_Train_Error'] = final_train_error\n",
    "        metrics['Final_Val_Error'] = final_val_error\n",
    "        metrics['Train_Val_Gap'] = train_val_gap\n",
    "        metrics['Dropout_Rate'] = dropout_rate\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  RMSE:           ${metrics['RMSE']:,.2f}\")\n",
    "        print(f\"  MAE:            ${metrics['MAE']:,.2f}\")\n",
    "        print(f\"  RÂ²:             {metrics['R2']:.4f}\")\n",
    "        print(f\"  MAPE:           {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"  Train Error:    {final_train_error:.6f}\")\n",
    "        print(f\"  Val Error:      {final_val_error:.6f}\")\n",
    "        print(f\"  Train-Val Gap:  {train_val_gap:.6f}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DROPOUT REGULARIZATION - SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df[['Dropout_Rate', 'RMSE', 'MAE', 'R2', 'MAPE', 'Train_Val_Gap']].to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = results_df['RMSE'].idxmin()\n",
    "    best_dropout = results_df.loc[best_idx, 'Dropout_Rate']\n",
    "    \n",
    "    print(f\"\\nğŸ† BEST DROPOUT RATE: {best_dropout}\")\n",
    "    print(f\"   RMSE: ${results_df.loc[best_idx, 'RMSE']:,.2f}\")\n",
    "    print(f\"   RÂ²:   {results_df.loc[best_idx, 'R2']:.4f}\")\n",
    "    print(f\"   Train-Val Gap: {results_df.loc[best_idx, 'Train_Val_Gap']:.6f}\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"â”€\"*80)\n",
    "    print(\"ANALYSIS:\")\n",
    "    baseline_rmse = results_df[results_df['Dropout_Rate'] == 0.0]['RMSE'].values[0]\n",
    "    best_rmse = results_df.loc[best_idx, 'RMSE']\n",
    "    improvement = ((baseline_rmse - best_rmse) / baseline_rmse) * 100\n",
    "    \n",
    "    print(f\"  â€¢ Baseline (no dropout) RMSE: ${baseline_rmse:,.2f}\")\n",
    "    print(f\"  â€¢ Best Dropout RMSE:           ${best_rmse:,.2f}\")\n",
    "    print(f\"  â€¢ Improvement:                 {improvement:.2f}%\")\n",
    "    \n",
    "    baseline_gap = results_df[results_df['Dropout_Rate'] == 0.0]['Train_Val_Gap'].values[0]\n",
    "    best_gap = results_df.loc[best_idx, 'Train_Val_Gap']\n",
    "    gap_reduction = ((baseline_gap - best_gap) / baseline_gap) * 100\n",
    "    print(f\"  â€¢ Train-Val Gap reduction:     {gap_reduction:.2f}%\")\n",
    "    print(\"â”€\"*80)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(\"dropout_regularization_results.csv\", index=False)\n",
    "    print(\"\\nâœ“ Results saved to: dropout_regularization_results.csv\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def study_combined_regularization(TRAIN_CSV, CSV_VAL, CSV_TEST, activation=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Study 3: Combined L2 + Dropout\n",
    "    Test combinations of both regularization techniques\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"STUDY 3: COMBINED L2 + DROPOUT REGULARIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nCombining both techniques can leverage their complementary strengths:\")\n",
    "    print(\"  â€¢ L2: Penalizes large weights\")\n",
    "    print(\"  â€¢ Dropout: Prevents neuron co-adaptation\\n\")\n",
    "    \n",
    "    # Test different combinations\n",
    "    combinations = [\n",
    "        (0.001, 0.1, \"Light regularization\"),\n",
    "        (0.001, 0.2, \"Moderate regularization\"),\n",
    "        (0.01, 0.2, \"Strong L2 + moderate dropout\"),\n",
    "        (0.001, 0.3, \"Light L2 + strong dropout\"),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    deeper_layers = [277, 64, 32, 16, 1]\n",
    "    \n",
    "    for l2_lambda, dropout_rate, description in combinations:\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Testing: {description}\")\n",
    "        print(f\"  L2 Lambda = {l2_lambda}, Dropout Rate = {dropout_rate}\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        # Run training\n",
    "        nn, pid_test, y_test, preds_test, trainer = run_training(\n",
    "            activation=activation,\n",
    "            verbose=False,\n",
    "            epochs=200,\n",
    "            CSV_TRAIN=TRAIN_CSV,\n",
    "            CSV_VAL=CSV_VAL,\n",
    "            CSV_TEST=CSV_TEST,\n",
    "            layers=deeper_layers,\n",
    "            lr=0.001,\n",
    "            l2_lambda=l2_lambda,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test, preds_test, description)\n",
    "        \n",
    "        # Get training history\n",
    "        loss_df = trainer.loss_epochs()\n",
    "        final_train_error = loss_df['Train Error'].iloc[-1]\n",
    "        final_val_error = loss_df['Val Error'].iloc[-1]\n",
    "        train_val_gap = abs(final_val_error - final_train_error)\n",
    "        \n",
    "        metrics['Final_Train_Error'] = final_train_error\n",
    "        metrics['Final_Val_Error'] = final_val_error\n",
    "        metrics['Train_Val_Gap'] = train_val_gap\n",
    "        metrics['L2_Lambda'] = l2_lambda\n",
    "        metrics['Dropout_Rate'] = dropout_rate\n",
    "        metrics['Description'] = description\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  RMSE:           ${metrics['RMSE']:,.2f}\")\n",
    "        print(f\"  MAE:            ${metrics['MAE']:,.2f}\")\n",
    "        print(f\"  RÂ²:             {metrics['R2']:.4f}\")\n",
    "        print(f\"  MAPE:           {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"  Train Error:    {final_train_error:.6f}\")\n",
    "        print(f\"  Val Error:      {final_val_error:.6f}\")\n",
    "        print(f\"  Train-Val Gap:  {train_val_gap:.6f}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMBINED REGULARIZATION - SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df[['Description', 'L2_Lambda', 'Dropout_Rate', 'RMSE', 'R2', 'Train_Val_Gap']].to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = results_df['RMSE'].idxmin()\n",
    "    \n",
    "    print(f\"\\nğŸ† BEST COMBINATION:\")\n",
    "    print(f\"   {results_df.loc[best_idx, 'Description']}\")\n",
    "    print(f\"   L2 Lambda: {results_df.loc[best_idx, 'L2_Lambda']}\")\n",
    "    print(f\"   Dropout Rate: {results_df.loc[best_idx, 'Dropout_Rate']}\")\n",
    "    print(f\"   RMSE: ${results_df.loc[best_idx, 'RMSE']:,.2f}\")\n",
    "    print(f\"   RÂ²:   {results_df.loc[best_idx, 'R2']:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(\"combined_regularization_results.csv\", index=False)\n",
    "    print(\"\\nâœ“ Results saved to: combined_regularization_results.csv\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, name=\"Model\"):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f6a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STUDY 1: L2 REGULARIZATION (WEIGHT DECAY)\n",
      "================================================================================\n",
      "\n",
      "L2 regularization adds a penalty term: Loss = MSE + (Î»/2) Ã— Î£(wÂ²)\n",
      "This prevents overfitting by penalizing large weights.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing L2_lambda = 0.0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.006990, Val Error: 0.006112\n",
      "Epoch 40/200 - Train Error: 0.006881, Val Error: 0.006012\n",
      "Epoch 60/200 - Train Error: 0.006309, Val Error: 0.005486\n",
      "Epoch 80/200 - Train Error: 0.004256, Val Error: 0.003624\n",
      "Epoch 100/200 - Train Error: 0.001957, Val Error: 0.001735\n",
      "Epoch 120/200 - Train Error: 0.001446, Val Error: 0.001389\n",
      "Epoch 140/200 - Train Error: 0.001225, Val Error: 0.001201\n",
      "Epoch 160/200 - Train Error: 0.001074, Val Error: 0.001057\n",
      "Epoch 180/200 - Train Error: 0.000967, Val Error: 0.000956\n",
      "Epoch 200/200 - Train Error: 0.000890, Val Error: 0.000889\n",
      "\n",
      "Results:\n",
      "  RMSE:           $27,205.52\n",
      "  MAE:            $19,208.85\n",
      "  RÂ²:             0.8611\n",
      "  MAPE:           10.11%\n",
      "  Train Error:    0.000890\n",
      "  Val Error:      0.000889\n",
      "  Train-Val Gap:  0.000001\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing L2_lambda = 0.0001\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.006981, Val Error: 0.006105\n",
      "Epoch 40/200 - Train Error: 0.006824, Val Error: 0.005962\n",
      "Epoch 60/200 - Train Error: 0.006094, Val Error: 0.005294\n",
      "Epoch 80/200 - Train Error: 0.004015, Val Error: 0.003423\n",
      "Epoch 100/200 - Train Error: 0.002004, Val Error: 0.001781\n",
      "Epoch 120/200 - Train Error: 0.001524, Val Error: 0.001479\n",
      "Epoch 140/200 - Train Error: 0.001317, Val Error: 0.001317\n",
      "Epoch 160/200 - Train Error: 0.001158, Val Error: 0.001166\n",
      "Epoch 180/200 - Train Error: 0.001035, Val Error: 0.001047\n",
      "Epoch 200/200 - Train Error: 0.000944, Val Error: 0.000964\n",
      "\n",
      "Results:\n",
      "  RMSE:           $28,683.25\n",
      "  MAE:            $20,330.51\n",
      "  RÂ²:             0.8456\n",
      "  MAPE:           10.64%\n",
      "  Train Error:    0.000944\n",
      "  Val Error:      0.000964\n",
      "  Train-Val Gap:  0.000020\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing L2_lambda = 0.001\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.006962, Val Error: 0.006093\n",
      "Epoch 40/200 - Train Error: 0.006178, Val Error: 0.005352\n",
      "Epoch 60/200 - Train Error: 0.002058, Val Error: 0.001805\n",
      "Epoch 80/200 - Train Error: 0.001726, Val Error: 0.001941\n",
      "Epoch 100/200 - Train Error: 0.001760, Val Error: 0.002076\n",
      "Epoch 120/200 - Train Error: 0.001746, Val Error: 0.002034\n",
      "Epoch 140/200 - Train Error: 0.001953, Val Error: 0.002219\n",
      "Epoch 160/200 - Train Error: 0.002286, Val Error: 0.002553\n",
      "Epoch 180/200 - Train Error: 0.002659, Val Error: 0.002936\n",
      "Epoch 200/200 - Train Error: 0.003004, Val Error: 0.003298\n",
      "\n",
      "Results:\n",
      "  RMSE:           $49,472.97\n",
      "  MAE:            $35,007.77\n",
      "  RÂ²:             0.5407\n",
      "  MAPE:           21.28%\n",
      "  Train Error:    0.003004\n",
      "  Val Error:      0.003298\n",
      "  Train-Val Gap:  0.000294\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing L2_lambda = 0.01\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.005994, Val Error: 0.007456\n",
      "Epoch 40/200 - Train Error: 0.009904, Val Error: 0.010306\n",
      "Epoch 60/200 - Train Error: 0.009400, Val Error: 0.009990\n",
      "Epoch 80/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "Epoch 100/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "Epoch 120/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "Epoch 140/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "Epoch 160/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "Epoch 180/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "Epoch 200/200 - Train Error: 0.009381, Val Error: 0.009976\n",
      "\n",
      "Results:\n",
      "  RMSE:           $97,684.63\n",
      "  MAE:            $71,202.71\n",
      "  RÂ²:             -0.7906\n",
      "  MAPE:           43.47%\n",
      "  Train Error:    0.009381\n",
      "  Val Error:      0.009976\n",
      "  Train-Val Gap:  0.000595\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing L2_lambda = 0.1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.031638, Val Error: 0.031966\n",
      "Epoch 40/200 - Train Error: 0.031638, Val Error: 0.031966\n",
      "Epoch 60/200 - Train Error: 0.031638, Val Error: 0.031966\n",
      "Epoch 80/200 - Train Error: 0.031638, Val Error: 0.031966\n",
      "Epoch 100/200 - Train Error: 0.031638, Val Error: 0.031966\n",
      "Epoch 120/200 - Train Error: 0.031638, Val Error: 0.031966\n",
      "Epoch 140/200 - Train Error: 0.031638, Val Error: 0.031966\n"
     ]
    }
   ],
   "source": [
    "l2_results = study_l2_regularization(\n",
    "    TRAIN_CSV,\n",
    "    VAL_CSV,\n",
    "    TEST_CSV,\n",
    "    activation=\"sigmoid\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b44cec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "STUDY 2: DROPOUT REGULARIZATION\n",
      "================================================================================\n",
      "\n",
      "Dropout randomly deactivates neurons during training.\n",
      "This prevents co-adaptation and acts as an ensemble method.\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing Dropout Rate = 0.0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007066, Val Error: 0.006215\n",
      "Epoch 40/200 - Train Error: 0.007062, Val Error: 0.006207\n",
      "Epoch 60/200 - Train Error: 0.007055, Val Error: 0.006194\n",
      "Epoch 80/200 - Train Error: 0.006991, Val Error: 0.006125\n",
      "Epoch 100/200 - Train Error: 0.001255, Val Error: 0.001224\n",
      "Epoch 120/200 - Train Error: 0.000765, Val Error: 0.000784\n",
      "Epoch 140/200 - Train Error: 0.000663, Val Error: 0.000730\n",
      "Epoch 160/200 - Train Error: 0.000619, Val Error: 0.000730\n",
      "Epoch 180/200 - Train Error: 0.000595, Val Error: 0.000731\n",
      "Epoch 200/200 - Train Error: 0.000577, Val Error: 0.000727\n",
      "\n",
      "Results:\n",
      "  RMSE:           $19,873.23\n",
      "  MAE:            $14,334.09\n",
      "  RÂ²:             0.9259\n",
      "  MAPE:           7.91%\n",
      "  Train Error:    0.000577\n",
      "  Val Error:      0.000727\n",
      "  Train-Val Gap:  0.000150\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing Dropout Rate = 0.1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007126, Val Error: 0.006208\n",
      "Epoch 40/200 - Train Error: 0.007065, Val Error: 0.006184\n",
      "Epoch 60/200 - Train Error: 0.007069, Val Error: 0.006197\n",
      "Epoch 80/200 - Train Error: 0.007064, Val Error: 0.006195\n",
      "Epoch 100/200 - Train Error: 0.007045, Val Error: 0.006192\n",
      "Epoch 120/200 - Train Error: 0.007054, Val Error: 0.006172\n",
      "Epoch 140/200 - Train Error: 0.007054, Val Error: 0.006172\n",
      "Epoch 160/200 - Train Error: 0.007040, Val Error: 0.006164\n",
      "Epoch 180/200 - Train Error: 0.007036, Val Error: 0.006162\n",
      "Epoch 200/200 - Train Error: 0.007029, Val Error: 0.006148\n",
      "\n",
      "Results:\n",
      "  RMSE:           $75,181.35\n",
      "  MAE:            $57,916.48\n",
      "  RÂ²:             -0.0606\n",
      "  MAPE:           33.48%\n",
      "  Train Error:    0.007029\n",
      "  Val Error:      0.006148\n",
      "  Train-Val Gap:  0.000881\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing Dropout Rate = 0.2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007056, Val Error: 0.006190\n",
      "Epoch 40/200 - Train Error: 0.007069, Val Error: 0.006190\n",
      "Epoch 60/200 - Train Error: 0.007055, Val Error: 0.006203\n",
      "Epoch 80/200 - Train Error: 0.007064, Val Error: 0.006186\n",
      "Epoch 100/200 - Train Error: 0.007060, Val Error: 0.006180\n",
      "Epoch 120/200 - Train Error: 0.007050, Val Error: 0.006175\n",
      "Epoch 140/200 - Train Error: 0.007055, Val Error: 0.006171\n",
      "Epoch 160/200 - Train Error: 0.007048, Val Error: 0.006165\n",
      "Epoch 180/200 - Train Error: 0.007019, Val Error: 0.006149\n",
      "Epoch 200/200 - Train Error: 0.007053, Val Error: 0.006155\n",
      "\n",
      "Results:\n",
      "  RMSE:           $75,243.00\n",
      "  MAE:            $57,959.79\n",
      "  RÂ²:             -0.0624\n",
      "  MAPE:           33.50%\n",
      "  Train Error:    0.007053\n",
      "  Val Error:      0.006155\n",
      "  Train-Val Gap:  0.000898\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing Dropout Rate = 0.3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007093, Val Error: 0.006207\n",
      "Epoch 40/200 - Train Error: 0.007043, Val Error: 0.006192\n",
      "Epoch 60/200 - Train Error: 0.007061, Val Error: 0.006197\n",
      "Epoch 80/200 - Train Error: 0.007072, Val Error: 0.006182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dropout_results = \u001b[43mstudy_dropout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTRAIN_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mVAL_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTEST_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msigmoid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mstudy_dropout\u001b[39m\u001b[34m(TRAIN_CSV, CSV_VAL, CSV_TEST, activation)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mâ”€\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Run training with specific dropout rate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m nn, pid_test, y_test, preds_test, trainer = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCSV_TRAIN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCSV_VAL\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_VAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCSV_TEST\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_TEST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeeper_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use deeper network\u001b[39;49;00m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Slightly lower LR for dropout\u001b[39;49;00m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml2_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No L2 for this study\u001b[39;49;00m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m    184\u001b[39m metrics = calculate_metrics(y_test, preds_test, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDropout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Master/Neural and Evolutionary Computation/Activity-1/src/part_2/main.py:192\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(activation, verbose, CSV_TRAIN, CSV_VAL, CSV_TEST, layers, epochs, lr, momentum, val_percent, scale, log_target, l2_lambda, dropout_rate)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLog Target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.log_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m nn, pid_test, y_test, preds_test, test_error = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCSV_TRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCSV_VAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCSV_TEST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    197\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Master/Neural and Evolutionary Computation/Activity-1/src/part_2/train_housing_bp.py:153\u001b[39m, in \u001b[36mTrainBP.train\u001b[39m\u001b[34m(self, train_path, val_path, test_path, target, verbose)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Initialize and fit network with regularization\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mself\u001b[39m.nn = NeuralNet(\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.layers,\n\u001b[32m    144\u001b[39m     epochs=\u001b[38;5;28mself\u001b[39m.epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m     dropout_rate=\u001b[38;5;28mself\u001b[39m.dropout_rate,  \u001b[38;5;66;03m# Dropout\u001b[39;00m\n\u001b[32m    152\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m    156\u001b[39m preds_test = \u001b[38;5;28mself\u001b[39m.nn.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Master/Neural and Evolutionary Computation/Activity-1/src/part_2/neural_net.py:178\u001b[39m, in \u001b[36mNeuralNet.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    176\u001b[39m train_error = \u001b[32m0\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xi_sample, yi_sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m.backward(yi_sample)\n\u001b[32m    180\u001b[39m     train_error += \u001b[32m0.5\u001b[39m * np.sum(\n\u001b[32m    181\u001b[39m         (yi_sample.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) - \u001b[38;5;28mself\u001b[39m.xi[-\u001b[32m1\u001b[39m]) ** \u001b[32m2\u001b[39m\n\u001b[32m    182\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Master/Neural and Evolutionary Computation/Activity-1/src/part_2/neural_net.py:114\u001b[39m, in \u001b[36mNeuralNet.forward\u001b[39m\u001b[34m(self, xi_input, training)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Dropout applied to activations\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m < l < \u001b[38;5;28mself\u001b[39m.L - \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout_rate > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     mask = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     mask = mask / (\u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.dropout_rate)\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mself\u001b[39m.dropout_masks[l] = mask\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dropout_results = study_dropout(\n",
    "    TRAIN_CSV,\n",
    "    VAL_CSV,\n",
    "    TEST_CSV,\n",
    "    activation=\"sigmoid\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6a10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "STUDY 3: COMBINED L2 + DROPOUT REGULARIZATION\n",
      "================================================================================\n",
      "\n",
      "Combining both techniques can leverage their complementary strengths:\n",
      "  â€¢ L2: Penalizes large weights\n",
      "  â€¢ Dropout: Prevents neuron co-adaptation\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: Light regularization\n",
      "  L2 Lambda = 0.001, Dropout Rate = 0.1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007116, Val Error: 0.006135\n",
      "Epoch 40/200 - Train Error: 0.007184, Val Error: 0.006134\n",
      "Epoch 60/200 - Train Error: 0.007137, Val Error: 0.006134\n",
      "Epoch 80/200 - Train Error: 0.007083, Val Error: 0.006121\n",
      "Epoch 100/200 - Train Error: 0.007147, Val Error: 0.006068\n",
      "Epoch 120/200 - Train Error: 0.006791, Val Error: 0.005795\n",
      "Epoch 140/200 - Train Error: 0.005326, Val Error: 0.004250\n",
      "Epoch 160/200 - Train Error: 0.003042, Val Error: 0.002048\n",
      "Epoch 180/200 - Train Error: 0.003062, Val Error: 0.002337\n",
      "Epoch 200/200 - Train Error: 0.002952, Val Error: 0.002345\n",
      "\n",
      "Results:\n",
      "  RMSE:           $39,599.24\n",
      "  MAE:            $30,307.48\n",
      "  RÂ²:             0.7057\n",
      "  MAPE:           17.92%\n",
      "  Train Error:    0.002952\n",
      "  Val Error:      0.002345\n",
      "  Train-Val Gap:  0.000607\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: Moderate regularization\n",
      "  L2 Lambda = 0.001, Dropout Rate = 0.2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007140, Val Error: 0.006133\n",
      "Epoch 40/200 - Train Error: 0.007277, Val Error: 0.006133\n",
      "Epoch 60/200 - Train Error: 0.007147, Val Error: 0.006135\n",
      "Epoch 80/200 - Train Error: 0.007162, Val Error: 0.006131\n",
      "Epoch 100/200 - Train Error: 0.007088, Val Error: 0.006122\n",
      "Epoch 120/200 - Train Error: 0.007114, Val Error: 0.006095\n",
      "Epoch 140/200 - Train Error: 0.006982, Val Error: 0.005989\n",
      "Epoch 160/200 - Train Error: 0.006617, Val Error: 0.005610\n",
      "Epoch 180/200 - Train Error: 0.005843, Val Error: 0.004497\n",
      "Epoch 200/200 - Train Error: 0.005233, Val Error: 0.003466\n",
      "\n",
      "Results:\n",
      "  RMSE:           $57,054.73\n",
      "  MAE:            $40,362.94\n",
      "  RÂ²:             0.3892\n",
      "  MAPE:           22.54%\n",
      "  Train Error:    0.005233\n",
      "  Val Error:      0.003466\n",
      "  Train-Val Gap:  0.001767\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: Strong L2 + moderate dropout\n",
      "  L2 Lambda = 0.01, Dropout Rate = 0.2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.009485, Val Error: 0.006513\n",
      "Epoch 40/200 - Train Error: 0.032011, Val Error: 0.032261\n",
      "Epoch 60/200 - Train Error: 0.033882, Val Error: 0.032757\n",
      "Epoch 80/200 - Train Error: 0.034713, Val Error: 0.034196\n",
      "Epoch 100/200 - Train Error: 0.035184, Val Error: 0.034194\n",
      "Epoch 120/200 - Train Error: 0.035234, Val Error: 0.034193\n",
      "Epoch 140/200 - Train Error: 0.035519, Val Error: 0.034193\n",
      "Epoch 160/200 - Train Error: 0.035284, Val Error: 0.034193\n",
      "Epoch 180/200 - Train Error: 0.034838, Val Error: 0.034193\n",
      "Epoch 200/200 - Train Error: 0.036762, Val Error: 0.034194\n",
      "\n",
      "Results:\n",
      "  RMSE:           $171,964.81\n",
      "  MAE:            $155,696.36\n",
      "  RÂ²:             -4.5491\n",
      "  MAPE:           116.53%\n",
      "  Train Error:    0.036762\n",
      "  Val Error:      0.034194\n",
      "  Train-Val Gap:  0.002569\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing: Light L2 + strong dropout\n",
      "  L2 Lambda = 0.001, Dropout Rate = 0.3\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Epoch 20/200 - Train Error: 0.007261, Val Error: 0.006138\n",
      "Epoch 40/200 - Train Error: 0.007289, Val Error: 0.006131\n",
      "Epoch 60/200 - Train Error: 0.007113, Val Error: 0.006127\n",
      "Epoch 80/200 - Train Error: 0.007102, Val Error: 0.006126\n",
      "Epoch 100/200 - Train Error: 0.007060, Val Error: 0.006123\n",
      "Epoch 120/200 - Train Error: 0.007051, Val Error: 0.006112\n",
      "Epoch 140/200 - Train Error: 0.007042, Val Error: 0.006067\n",
      "Epoch 160/200 - Train Error: 0.006897, Val Error: 0.005907\n",
      "Epoch 180/200 - Train Error: 0.006646, Val Error: 0.005488\n",
      "Epoch 200/200 - Train Error: 0.006466, Val Error: 0.004935\n",
      "\n",
      "Results:\n",
      "  RMSE:           $69,014.79\n",
      "  MAE:            $51,708.30\n",
      "  RÂ²:             0.1062\n",
      "  MAPE:           29.06%\n",
      "  Train Error:    0.006466\n",
      "  Val Error:      0.004935\n",
      "  Train-Val Gap:  0.001531\n",
      "\n",
      "================================================================================\n",
      "COMBINED REGULARIZATION - SUMMARY TABLE\n",
      "================================================================================\n",
      "                 Description  L2_Lambda  Dropout_Rate          RMSE        R2  Train_Val_Gap\n",
      "        Light regularization      0.001           0.1  39599.237830  0.705748       0.000607\n",
      "     Moderate regularization      0.001           0.2  57054.733109  0.389158       0.001767\n",
      "Strong L2 + moderate dropout      0.010           0.2 171964.813812 -4.549123       0.002569\n",
      "   Light L2 + strong dropout      0.001           0.3  69014.785557  0.106222       0.001531\n",
      "\n",
      "ğŸ† BEST COMBINATION:\n",
      "   Light regularization\n",
      "   L2 Lambda: 0.001\n",
      "   Dropout Rate: 0.1\n",
      "   RMSE: $39,599.24\n",
      "   RÂ²:   0.7057\n",
      "\n",
      "âœ“ Results saved to: combined_regularization_results.csv\n"
     ]
    }
   ],
   "source": [
    "combined_results = study_combined_regularization(\n",
    "    TRAIN_CSV,\n",
    "    VAL_CSV,\n",
    "    TEST_CSV,\n",
    "    activation=\"sigmoid\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd3253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
